{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os, sys\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softloss(nn.Module):\n",
    "    def __init__(self,T=4,loss_portion=[1,0,0]) -> None:\n",
    "        '''\n",
    "        T: temperature\n",
    "        loss_portion: KLD, cosine, mse\n",
    "        '''\n",
    "        super(Softloss,self).__init__()\n",
    "        self.T=T\n",
    "        self.portion=loss_portion\n",
    "    def forward(self,x,y):\n",
    "        soft_x=F.log_softmax(x/self.T,dim=-1)\n",
    "        soft_y=F.softmax(y/self.T,dim=-1)\n",
    "        loss=self.portion[0]*F.kl_div(soft_x,soft_y,reduction=\"batchmean\")\\\n",
    "            +self.portion[1]*F.cosine_embedding_loss(soft_x,soft_y,torch.ones(soft_x.shape[0]).to(soft_x.device))\\\n",
    "            +self.portion[2]*F.mse_loss(soft_x,soft_y)\n",
    "        return loss*self.T*self.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock11(nn.Module):\n",
    "    def __init__(self,channel,filter_size=3):\n",
    "        super(ResBlock11, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channel, channel, filter_size,padding=1,bias=False)\n",
    "        nn.init.kaiming_normal_(self.conv1.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "        self.norm1 = nn.BatchNorm2d(channel)\n",
    "        self.norm3 = nn.BatchNorm2d(channel)\n",
    "    def forward(self,x):\n",
    "        out=F.leaky_relu(self.norm1(self.conv1(x)))\n",
    "        out=self.norm3(out+x)\n",
    "        return out\n",
    "\n",
    "class ResDownSampling11(nn.Module):\n",
    "    def __init__(self,channel,out_channel,filter_size=3):\n",
    "        super(ResDownSampling11, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channel, out_channel, filter_size,stride=2,padding=1,bias=False)\n",
    "        nn.init.kaiming_normal_(self.conv1.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "        self.norm1 = nn.BatchNorm2d(out_channel)\n",
    "        self.norm3 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv3 = nn.Conv2d(channel,out_channel,kernel_size=1,stride=2,bias=False)\n",
    "        nn.init.kaiming_normal_(self.conv3.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "        self.norm4 = nn.BatchNorm2d(out_channel)\n",
    "    def forward(self,x):\n",
    "        out=F.leaky_relu(self.norm1(self.conv1(x)))\n",
    "        x=F.leaky_relu(self.norm3(self.conv3(x)))\n",
    "        out=self.norm4(out+x)\n",
    "        return out\n",
    "class ResNetCIFAR11(nn.Module):\n",
    "    def __init__(self ):\n",
    "        super(ResNetCIFAR11, self).__init__()\n",
    "        self.inconv=nn.Conv2d(3, 16,3,padding=1,bias=False)\n",
    "        nn.init.xavier_normal_(self.inconv.weight)\n",
    "        self.res_block=nn.Sequential(self.inconv,\n",
    "                                    nn.BatchNorm2d(16),\n",
    "                                    nn.ReLU(),\n",
    "                                    ResBlock11(16),\n",
    "                                    ResBlock11(16),\n",
    "                                    ResBlock11(16),\n",
    "                                    ResDownSampling11(16,32),\n",
    "                                    ResBlock11(32),\n",
    "                                    ResBlock11(32),\n",
    "                                    ResDownSampling11(32,64),\n",
    "                                    ResBlock11(64),\n",
    "                                    ResBlock11(64),\n",
    "                                    nn.AvgPool2d(8),\n",
    "                                    nn.Flatten(),\n",
    "                                    nn.Linear(64*1*1,10))\n",
    "    def forward(self, x):\n",
    "        return self.res_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self,channel,filter_size=3):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channel, channel, filter_size,padding=1,bias=False)\n",
    "        nn.init.kaiming_normal_(self.conv1.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "        self.norm1 = nn.BatchNorm2d(channel)\n",
    "        self.conv2 = nn.Conv2d(channel, channel, filter_size,padding=1,bias=False)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "        self.norm2 = nn.BatchNorm2d(channel)\n",
    "        self.norm3 = nn.BatchNorm2d(channel)\n",
    "    def forward(self,x):\n",
    "        out=F.leaky_relu(self.norm1(self.conv1(x)))\n",
    "        out=F.leaky_relu(self.norm2(self.conv2(out)))\n",
    "        out=self.norm3(out+x)\n",
    "        return out\n",
    "\n",
    "class ResDownSampling(nn.Module):\n",
    "    def __init__(self,channel,out_channel,filter_size=3):\n",
    "        super(ResDownSampling, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channel, channel, filter_size,stride=2,padding=1,bias=False)\n",
    "        nn.init.kaiming_normal_(self.conv1.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "        self.norm1 = nn.BatchNorm2d(channel)\n",
    "        self.conv2 = nn.Conv2d(channel, out_channel, filter_size,padding=1,bias=False)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "        self.norm2 = nn.BatchNorm2d(out_channel)\n",
    "        self.norm3 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv3 = nn.Conv2d(channel,out_channel,kernel_size=1,stride=2,bias=False)\n",
    "        nn.init.kaiming_normal_(self.conv3.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "        self.norm4 = nn.BatchNorm2d(out_channel)\n",
    "    def forward(self,x):\n",
    "        out=F.leaky_relu(self.norm1(self.conv1(x)))\n",
    "        out=F.leaky_relu(self.norm2(self.conv2(out)))\n",
    "        x=F.leaky_relu(self.norm3(self.conv3(x)))\n",
    "        out=self.norm4(out+x)\n",
    "        return out\n",
    "class ResNetCIFAR(nn.Module):\n",
    "    def __init__(self ):\n",
    "        super(ResNetCIFAR, self).__init__()\n",
    "        self.inconv=nn.Conv2d(3, 16,3,padding=1,bias=False)\n",
    "        nn.init.xavier_normal_(self.inconv.weight)\n",
    "        self.res_block=nn.Sequential(self.inconv,\n",
    "                                    nn.BatchNorm2d(16),\n",
    "                                    nn.ReLU(),\n",
    "                                    ResBlock(16),\n",
    "                                    ResBlock(16),\n",
    "                                    ResBlock(16),\n",
    "                                    ResDownSampling(16,32),\n",
    "                                    ResBlock(32),\n",
    "                                    ResBlock(32),\n",
    "                                    ResDownSampling(32,64),\n",
    "                                    ResBlock(64),\n",
    "                                    ResBlock(64),\n",
    "                                    nn.AvgPool2d(8),\n",
    "                                    nn.Flatten(),\n",
    "                                    nn.Linear(64*1*1,10))\n",
    "    def forward(self, x):\n",
    "        return self.res_block(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# useful libraries\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#############################################\n",
    "# your code here\n",
    "# specify preprocessing function\n",
    "transform = transforms.Compose(\n",
    "    (\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    )\n",
    ")\n",
    "transform_train = transforms.Compose(\n",
    "    (\n",
    "    \n",
    "    transforms.RandomCrop((32,32),padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    #transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    \n",
    "    #\n",
    "    #transforms.ColorJitter(0.2,0,0)\n",
    "    \n",
    "    )\n",
    ")\n",
    "\n",
    "transform_val = transform\n",
    "#############################################\n",
    "# do NOT change these\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# a few arguments, do NOT change these\n",
    "DATA_ROOT = \"./data\"\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "VAL_BATCH_SIZE = 100\n",
    "\n",
    "#############################################\n",
    "# your code here\n",
    "# construct dataset\n",
    "train_set = CIFAR10(\n",
    "    root=DATA_ROOT, \n",
    "    train=True, \n",
    "    download=True,\n",
    "    transform=transform_train    # your code\n",
    ")\n",
    "\n",
    "val_set = CIFAR10(\n",
    "    root=DATA_ROOT, \n",
    "    train=False, \n",
    "    download=True,\n",
    "    transform=transform_val    # your code\n",
    ")\n",
    "\n",
    "# construct dataloader\n",
    "train_loader = DataLoader(\n",
    "    train_set, \n",
    "    batch_size=TRAIN_BATCH_SIZE,  # your code\n",
    "    shuffle=True,     # your code\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set, \n",
    "    batch_size=VAL_BATCH_SIZE,  # your code\n",
    "    shuffle=False,     # your code\n",
    "    num_workers=2\n",
    ")\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "student=ResNetCIFAR11()\n",
    "student=student.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(T,portion,alpha,train_loader,val_loader,EPOCHS=200):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    student=ResNetCIFAR()\n",
    "    student=student.to(device)\n",
    "    model=student\n",
    "    teacher=ResNetCIFAR11()\n",
    "    state=torch.load(\"./saved_model/res11_8968_sgd.pth\")[\"state_dict\"]\n",
    "    teacher.load_state_dict(state)\n",
    "    teacher=teacher.to(device)\n",
    "    state=None\n",
    "    # some hyperparameters\n",
    "    # total number of training epochs\n",
    "    teacher.eval()\n",
    "    # hyperparameters, do NOT change right now\n",
    "    # initial learning rate\n",
    "    INITIAL_LR = 0.1\n",
    "\n",
    "    # momentum for optimizer\n",
    "    MOMENTUM = 0.9\n",
    "\n",
    "    # L2 regularization strength\n",
    "    REG = 0.0004\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Add optimizer\n",
    "    optimizer = optim.SGD(student.parameters(),weight_decay=REG,lr=INITIAL_LR,momentum=MOMENTUM,nesterov=True)\n",
    "    soft_criterion=Softloss(T,portion)\n",
    "    # the folder where the trained model is saved\n",
    "    CHECKPOINT_FOLDER = \"./tmp_model\"\n",
    "    DECAY_EPOCHS=1\n",
    "    DECAY=0.1\n",
    "    # start the training/validation process\n",
    "    # the process should take about 5 minutes on a GTX 1070-Ti\n",
    "    # if the code is written efficiently.\n",
    "    best_val_acc = 0\n",
    "    current_learning_rate = INITIAL_LR\n",
    "    \n",
    "    print(\"==> Training starts!\")\n",
    "    print(\"=\"*50)\n",
    "    for i in range(0, EPOCHS):\n",
    "        # handle the learning rate scheduler.\n",
    "        \n",
    "        if i in [70,140]:\n",
    "            current_learning_rate = current_learning_rate * DECAY\n",
    "        \n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = current_learning_rate\n",
    "            #print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
    "        \n",
    "        #######################\n",
    "        # your code here\n",
    "        # switch to train mode\n",
    "        model.train()\n",
    "        \n",
    "        #######################\n",
    "        \n",
    "        print(\"Epoch %d:\" %i)\n",
    "        # this help you compute the training accuracy\n",
    "        total_examples = 0\n",
    "        correct_examples = 0\n",
    "\n",
    "        train_loss = 0 # track training loss if you want\n",
    "        loader=train_loader\n",
    "        \n",
    "        # Train the model for 1 epoch.\n",
    "        for batch_idx, (inputs, targets) in enumerate(loader):\n",
    "            ####################################\n",
    "            # your code here\n",
    "            # copy inputs to device\n",
    "            inputs=inputs.to(device)\n",
    "            targets=targets.to(device).long()\n",
    "\n",
    "            \n",
    "            # compute the output and loss\n",
    "            out=model(inputs)\n",
    "            with torch.no_grad():\n",
    "                soft_target=teacher(inputs)\n",
    "            loss=(1-alpha)*criterion(out,targets)+alpha*soft_criterion(out,soft_target)\n",
    "            \n",
    "            # zero the gradient\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            \n",
    "            # apply gradient and update the weights\n",
    "            optimizer.step()\n",
    "            train_loss+=loss.item()\n",
    "            \n",
    "            # count the number of correctly predicted samples in the current batch\n",
    "            correct_examples+=torch.sum(out.argmax(-1)==targets).item()\n",
    "            ####################################\n",
    "        total_examples=len(train_loader.dataset)      \n",
    "        avg_loss = train_loss / len(train_loader)\n",
    "        avg_acc = correct_examples / total_examples\n",
    "        print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
    "\n",
    "        # Validate on the validation dataset\n",
    "        #######################\n",
    "        # your code here\n",
    "        # switch to eval mode\n",
    "        model.eval()\n",
    "        \n",
    "        #######################\n",
    "\n",
    "        # this help you compute the validation accuracy\n",
    "        total_examples = 0\n",
    "        correct_examples = 0\n",
    "        \n",
    "        val_loss = 0 # again, track the validation loss if you want\n",
    "\n",
    "        # disable gradient during validation, which can save GPU memory\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "                ####################################\n",
    "                # your code here\n",
    "                # copy inputs to device\n",
    "                inputs=inputs.to(device)\n",
    "                targets=targets.to(device).long()\n",
    "                # compute the output and loss\n",
    "                out=model(inputs)\n",
    "                loss=criterion(out,targets)\n",
    "                # count the number of correctly predicted samples in the current batch\n",
    "                val_loss+=loss.item()\n",
    "                correct_examples+=torch.sum(out.argmax(-1)==targets).item()\n",
    "                \n",
    "                ####################################\n",
    "        total_examples=len(val_loader.dataset)\n",
    "        avg_loss = val_loss / len(val_loader)\n",
    "        avg_acc = correct_examples / total_examples\n",
    "        print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
    "        \n",
    "        # save the model checkpoint\n",
    "        if avg_acc > best_val_acc:\n",
    "            best_val_acc = avg_acc\n",
    "            if not os.path.exists(CHECKPOINT_FOLDER):\n",
    "                os.makedirs(CHECKPOINT_FOLDER)\n",
    "            print(\"Saving ...\")\n",
    "            state = {'state_dict': model.state_dict(),\n",
    "                    'epoch': i,\n",
    "                    }\n",
    "            torch.save(state, os.path.join(CHECKPOINT_FOLDER, str(T)+'_distilled.pth'))\n",
    "            \n",
    "\n",
    "    print(\"=\"*50)\n",
    "    print(f\"==> Optimization finished! Best validation accuracy: {best_val_acc:.4f}\")\n",
    "    return best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Training starts!\n",
      "==================================================\n",
      "Epoch 0:\n",
      "Training loss: 1.5067, Training accuracy: 0.4879\n",
      "Validation loss: 1.1423, Validation accuracy: 0.6079\n",
      "Saving ...\n",
      "Epoch 1:\n",
      "Training loss: 0.9481, Training accuracy: 0.6831\n",
      "Validation loss: 0.9367, Validation accuracy: 0.6774\n",
      "Saving ...\n",
      "Epoch 2:\n",
      "Training loss: 0.7466, Training accuracy: 0.7497\n",
      "Validation loss: 0.7266, Validation accuracy: 0.7511\n",
      "Saving ...\n",
      "Epoch 3:\n",
      "Training loss: 0.6561, Training accuracy: 0.7805\n",
      "Validation loss: 0.7169, Validation accuracy: 0.7559\n",
      "Saving ...\n",
      "Epoch 4:\n",
      "Training loss: 0.6055, Training accuracy: 0.7967\n",
      "Validation loss: 0.7179, Validation accuracy: 0.7580\n",
      "Saving ...\n",
      "Epoch 5:\n",
      "Training loss: 0.5713, Training accuracy: 0.8082\n",
      "Validation loss: 0.6357, Validation accuracy: 0.7928\n",
      "Saving ...\n",
      "Epoch 6:\n",
      "Training loss: 0.5473, Training accuracy: 0.8182\n",
      "Validation loss: 0.6555, Validation accuracy: 0.7812\n",
      "Epoch 7:\n",
      "Training loss: 0.5293, Training accuracy: 0.8237\n",
      "Validation loss: 0.6590, Validation accuracy: 0.7856\n",
      "Epoch 8:\n",
      "Training loss: 0.5160, Training accuracy: 0.8278\n",
      "Validation loss: 0.5835, Validation accuracy: 0.8013\n",
      "Saving ...\n",
      "Epoch 9:\n",
      "Training loss: 0.5011, Training accuracy: 0.8335\n",
      "Validation loss: 0.7909, Validation accuracy: 0.7290\n",
      "Epoch 10:\n",
      "Training loss: 0.4941, Training accuracy: 0.8366\n",
      "Validation loss: 0.5330, Validation accuracy: 0.8180\n",
      "Saving ...\n",
      "Epoch 11:\n",
      "Training loss: 0.4845, Training accuracy: 0.8403\n",
      "Validation loss: 0.5884, Validation accuracy: 0.8048\n",
      "Epoch 12:\n",
      "Training loss: 0.4764, Training accuracy: 0.8417\n",
      "Validation loss: 0.5643, Validation accuracy: 0.8150\n",
      "Epoch 13:\n",
      "Training loss: 0.4713, Training accuracy: 0.8453\n",
      "Validation loss: 0.6478, Validation accuracy: 0.7859\n",
      "Epoch 14:\n",
      "Training loss: 0.4631, Training accuracy: 0.8468\n",
      "Validation loss: 0.5841, Validation accuracy: 0.8075\n",
      "Epoch 15:\n",
      "Training loss: 0.4610, Training accuracy: 0.8461\n",
      "Validation loss: 0.5481, Validation accuracy: 0.8136\n",
      "Epoch 16:\n",
      "Training loss: 0.4512, Training accuracy: 0.8512\n",
      "Validation loss: 0.6388, Validation accuracy: 0.7892\n",
      "Epoch 17:\n",
      "Training loss: 0.4527, Training accuracy: 0.8501\n",
      "Validation loss: 0.5090, Validation accuracy: 0.8351\n",
      "Saving ...\n",
      "Epoch 18:\n",
      "Training loss: 0.4458, Training accuracy: 0.8550\n",
      "Validation loss: 0.5313, Validation accuracy: 0.8234\n",
      "Epoch 19:\n",
      "Training loss: 0.4402, Training accuracy: 0.8565\n",
      "Validation loss: 0.5491, Validation accuracy: 0.8204\n",
      "Epoch 20:\n",
      "Training loss: 0.4405, Training accuracy: 0.8550\n",
      "Validation loss: 0.6304, Validation accuracy: 0.7909\n",
      "Epoch 21:\n",
      "Training loss: 0.4340, Training accuracy: 0.8571\n",
      "Validation loss: 0.6224, Validation accuracy: 0.7958\n",
      "Epoch 22:\n",
      "Training loss: 0.4354, Training accuracy: 0.8584\n",
      "Validation loss: 0.5461, Validation accuracy: 0.8199\n",
      "Epoch 23:\n",
      "Training loss: 0.4291, Training accuracy: 0.8608\n",
      "Validation loss: 0.5281, Validation accuracy: 0.8240\n",
      "Epoch 24:\n",
      "Training loss: 0.4294, Training accuracy: 0.8595\n",
      "Validation loss: 0.4738, Validation accuracy: 0.8392\n",
      "Saving ...\n",
      "Epoch 25:\n",
      "Training loss: 0.4248, Training accuracy: 0.8604\n",
      "Validation loss: 0.5486, Validation accuracy: 0.8179\n",
      "Epoch 26:\n",
      "Training loss: 0.4279, Training accuracy: 0.8586\n",
      "Validation loss: 0.5070, Validation accuracy: 0.8288\n",
      "Epoch 27:\n",
      "Training loss: 0.4196, Training accuracy: 0.8645\n",
      "Validation loss: 0.5098, Validation accuracy: 0.8319\n",
      "Epoch 28:\n",
      "Training loss: 0.4215, Training accuracy: 0.8644\n",
      "Validation loss: 0.5176, Validation accuracy: 0.8281\n",
      "Epoch 29:\n",
      "Training loss: 0.4182, Training accuracy: 0.8633\n",
      "Validation loss: 0.5556, Validation accuracy: 0.8131\n",
      "Epoch 30:\n",
      "Training loss: 0.4144, Training accuracy: 0.8672\n",
      "Validation loss: 0.4995, Validation accuracy: 0.8356\n",
      "Epoch 31:\n",
      "Training loss: 0.4131, Training accuracy: 0.8665\n",
      "Validation loss: 0.5030, Validation accuracy: 0.8307\n",
      "Epoch 32:\n",
      "Training loss: 0.4173, Training accuracy: 0.8638\n",
      "Validation loss: 0.4855, Validation accuracy: 0.8407\n",
      "Saving ...\n",
      "Epoch 33:\n",
      "Training loss: 0.4111, Training accuracy: 0.8669\n",
      "Validation loss: 0.5542, Validation accuracy: 0.8136\n",
      "Epoch 34:\n",
      "Training loss: 0.4106, Training accuracy: 0.8677\n",
      "Validation loss: 0.5367, Validation accuracy: 0.8195\n",
      "Epoch 35:\n",
      "Training loss: 0.4135, Training accuracy: 0.8666\n",
      "Validation loss: 0.5105, Validation accuracy: 0.8261\n",
      "Epoch 36:\n",
      "Training loss: 0.4074, Training accuracy: 0.8682\n",
      "Validation loss: 0.5868, Validation accuracy: 0.8082\n",
      "Epoch 37:\n",
      "Training loss: 0.4106, Training accuracy: 0.8671\n",
      "Validation loss: 0.6163, Validation accuracy: 0.7992\n",
      "Epoch 38:\n",
      "Training loss: 0.4037, Training accuracy: 0.8692\n",
      "Validation loss: 0.5000, Validation accuracy: 0.8348\n",
      "Epoch 39:\n",
      "Training loss: 0.4083, Training accuracy: 0.8705\n",
      "Validation loss: 0.5197, Validation accuracy: 0.8304\n",
      "Epoch 40:\n",
      "Training loss: 0.4039, Training accuracy: 0.8709\n",
      "Validation loss: 0.5197, Validation accuracy: 0.8304\n",
      "Epoch 41:\n",
      "Training loss: 0.4080, Training accuracy: 0.8678\n",
      "Validation loss: 0.5135, Validation accuracy: 0.8311\n",
      "Epoch 42:\n",
      "Training loss: 0.4049, Training accuracy: 0.8698\n",
      "Validation loss: 0.4559, Validation accuracy: 0.8508\n",
      "Saving ...\n",
      "Epoch 43:\n",
      "Training loss: 0.4029, Training accuracy: 0.8706\n",
      "Validation loss: 0.4339, Validation accuracy: 0.8579\n",
      "Saving ...\n",
      "Epoch 44:\n",
      "Training loss: 0.4024, Training accuracy: 0.8705\n",
      "Validation loss: 0.4548, Validation accuracy: 0.8456\n",
      "Epoch 45:\n",
      "Training loss: 0.4004, Training accuracy: 0.8713\n",
      "Validation loss: 0.5254, Validation accuracy: 0.8264\n",
      "Epoch 46:\n",
      "Training loss: 0.4002, Training accuracy: 0.8712\n",
      "Validation loss: 0.6175, Validation accuracy: 0.7922\n",
      "Epoch 47:\n",
      "Training loss: 0.4045, Training accuracy: 0.8704\n",
      "Validation loss: 0.4473, Validation accuracy: 0.8528\n",
      "Epoch 48:\n",
      "Training loss: 0.3984, Training accuracy: 0.8703\n",
      "Validation loss: 0.4567, Validation accuracy: 0.8449\n",
      "Epoch 49:\n",
      "Training loss: 0.4040, Training accuracy: 0.8689\n",
      "Validation loss: 0.5153, Validation accuracy: 0.8296\n",
      "Epoch 50:\n",
      "Training loss: 0.3959, Training accuracy: 0.8728\n",
      "Validation loss: 0.5474, Validation accuracy: 0.8201\n",
      "Epoch 51:\n",
      "Training loss: 0.3961, Training accuracy: 0.8717\n",
      "Validation loss: 0.5895, Validation accuracy: 0.8004\n",
      "Epoch 52:\n",
      "Training loss: 0.3980, Training accuracy: 0.8700\n",
      "Validation loss: 0.4456, Validation accuracy: 0.8511\n",
      "Epoch 53:\n",
      "Training loss: 0.3967, Training accuracy: 0.8728\n",
      "Validation loss: 0.5396, Validation accuracy: 0.8267\n",
      "Epoch 54:\n",
      "Training loss: 0.3985, Training accuracy: 0.8710\n",
      "Validation loss: 0.5379, Validation accuracy: 0.8206\n",
      "Epoch 55:\n",
      "Training loss: 0.3959, Training accuracy: 0.8734\n",
      "Validation loss: 0.5957, Validation accuracy: 0.8011\n",
      "Epoch 56:\n",
      "Training loss: 0.3924, Training accuracy: 0.8750\n",
      "Validation loss: 0.4658, Validation accuracy: 0.8431\n",
      "Epoch 57:\n",
      "Training loss: 0.3963, Training accuracy: 0.8711\n",
      "Validation loss: 0.5689, Validation accuracy: 0.8053\n",
      "Epoch 58:\n",
      "Training loss: 0.3910, Training accuracy: 0.8753\n",
      "Validation loss: 0.5253, Validation accuracy: 0.8262\n",
      "Epoch 59:\n",
      "Training loss: 0.3982, Training accuracy: 0.8709\n",
      "Validation loss: 0.4492, Validation accuracy: 0.8524\n",
      "Epoch 60:\n",
      "Training loss: 0.3933, Training accuracy: 0.8744\n",
      "Validation loss: 0.4876, Validation accuracy: 0.8328\n",
      "Epoch 61:\n",
      "Training loss: 0.3953, Training accuracy: 0.8745\n",
      "Validation loss: 0.4995, Validation accuracy: 0.8352\n",
      "Epoch 62:\n",
      "Training loss: 0.3944, Training accuracy: 0.8741\n",
      "Validation loss: 0.4613, Validation accuracy: 0.8430\n",
      "Epoch 63:\n",
      "Training loss: 0.3925, Training accuracy: 0.8739\n",
      "Validation loss: 0.5916, Validation accuracy: 0.8038\n",
      "Epoch 64:\n",
      "Training loss: 0.3943, Training accuracy: 0.8732\n",
      "Validation loss: 0.5321, Validation accuracy: 0.8229\n",
      "Epoch 65:\n",
      "Training loss: 0.3930, Training accuracy: 0.8745\n",
      "Validation loss: 0.5217, Validation accuracy: 0.8248\n",
      "Epoch 66:\n",
      "Training loss: 0.3949, Training accuracy: 0.8741\n",
      "Validation loss: 0.4966, Validation accuracy: 0.8325\n",
      "Epoch 67:\n",
      "Training loss: 0.3920, Training accuracy: 0.8754\n",
      "Validation loss: 0.5001, Validation accuracy: 0.8352\n",
      "Epoch 68:\n",
      "Training loss: 0.3923, Training accuracy: 0.8749\n",
      "Validation loss: 0.5296, Validation accuracy: 0.8268\n",
      "Epoch 69:\n",
      "Training loss: 0.3887, Training accuracy: 0.8766\n",
      "Validation loss: 0.4665, Validation accuracy: 0.8474\n",
      "Epoch 70:\n",
      "Training loss: 0.2818, Training accuracy: 0.9139\n",
      "Validation loss: 0.2890, Validation accuracy: 0.9043\n",
      "Saving ...\n",
      "Epoch 71:\n",
      "Training loss: 0.2532, Training accuracy: 0.9261\n",
      "Validation loss: 0.2773, Validation accuracy: 0.9090\n",
      "Saving ...\n",
      "Epoch 72:\n",
      "Training loss: 0.2421, Training accuracy: 0.9314\n",
      "Validation loss: 0.2725, Validation accuracy: 0.9117\n",
      "Saving ...\n",
      "Epoch 73:\n",
      "Training loss: 0.2325, Training accuracy: 0.9353\n",
      "Validation loss: 0.2692, Validation accuracy: 0.9120\n",
      "Saving ...\n",
      "Epoch 74:\n",
      "Training loss: 0.2279, Training accuracy: 0.9376\n",
      "Validation loss: 0.2701, Validation accuracy: 0.9104\n",
      "Epoch 75:\n",
      "Training loss: 0.2217, Training accuracy: 0.9412\n",
      "Validation loss: 0.2658, Validation accuracy: 0.9132\n",
      "Saving ...\n",
      "Epoch 76:\n",
      "Training loss: 0.2173, Training accuracy: 0.9429\n",
      "Validation loss: 0.2580, Validation accuracy: 0.9160\n",
      "Saving ...\n",
      "Epoch 77:\n",
      "Training loss: 0.2140, Training accuracy: 0.9437\n",
      "Validation loss: 0.2587, Validation accuracy: 0.9143\n",
      "Epoch 78:\n",
      "Training loss: 0.2113, Training accuracy: 0.9449\n",
      "Validation loss: 0.2588, Validation accuracy: 0.9164\n",
      "Saving ...\n",
      "Epoch 79:\n",
      "Training loss: 0.2081, Training accuracy: 0.9476\n",
      "Validation loss: 0.2596, Validation accuracy: 0.9158\n",
      "Epoch 80:\n",
      "Training loss: 0.2076, Training accuracy: 0.9480\n",
      "Validation loss: 0.2592, Validation accuracy: 0.9146\n",
      "Epoch 81:\n",
      "Training loss: 0.2045, Training accuracy: 0.9493\n",
      "Validation loss: 0.2627, Validation accuracy: 0.9115\n",
      "Epoch 82:\n",
      "Training loss: 0.2008, Training accuracy: 0.9500\n",
      "Validation loss: 0.2588, Validation accuracy: 0.9142\n",
      "Epoch 83:\n",
      "Training loss: 0.2004, Training accuracy: 0.9518\n",
      "Validation loss: 0.2601, Validation accuracy: 0.9141\n",
      "Epoch 84:\n",
      "Training loss: 0.1974, Training accuracy: 0.9523\n",
      "Validation loss: 0.2671, Validation accuracy: 0.9111\n",
      "Epoch 85:\n",
      "Training loss: 0.1981, Training accuracy: 0.9522\n",
      "Validation loss: 0.2645, Validation accuracy: 0.9130\n",
      "Epoch 86:\n",
      "Training loss: 0.1975, Training accuracy: 0.9530\n",
      "Validation loss: 0.2614, Validation accuracy: 0.9141\n",
      "Epoch 87:\n",
      "Training loss: 0.1933, Training accuracy: 0.9546\n",
      "Validation loss: 0.2626, Validation accuracy: 0.9129\n",
      "Epoch 88:\n",
      "Training loss: 0.1936, Training accuracy: 0.9550\n",
      "Validation loss: 0.2660, Validation accuracy: 0.9117\n",
      "Epoch 89:\n",
      "Training loss: 0.1927, Training accuracy: 0.9550\n",
      "Validation loss: 0.2906, Validation accuracy: 0.9044\n",
      "Epoch 90:\n",
      "Training loss: 0.1912, Training accuracy: 0.9569\n",
      "Validation loss: 0.2656, Validation accuracy: 0.9124\n",
      "Epoch 91:\n",
      "Training loss: 0.1907, Training accuracy: 0.9555\n",
      "Validation loss: 0.2685, Validation accuracy: 0.9113\n",
      "Epoch 92:\n",
      "Training loss: 0.1914, Training accuracy: 0.9552\n",
      "Validation loss: 0.2657, Validation accuracy: 0.9111\n",
      "Epoch 93:\n",
      "Training loss: 0.1918, Training accuracy: 0.9553\n",
      "Validation loss: 0.2683, Validation accuracy: 0.9123\n",
      "Epoch 94:\n",
      "Training loss: 0.1919, Training accuracy: 0.9548\n",
      "Validation loss: 0.2718, Validation accuracy: 0.9117\n",
      "Epoch 95:\n",
      "Training loss: 0.1887, Training accuracy: 0.9564\n",
      "Validation loss: 0.2754, Validation accuracy: 0.9112\n",
      "Epoch 96:\n",
      "Training loss: 0.1899, Training accuracy: 0.9562\n",
      "Validation loss: 0.2852, Validation accuracy: 0.9074\n",
      "Epoch 97:\n",
      "Training loss: 0.1891, Training accuracy: 0.9575\n",
      "Validation loss: 0.2609, Validation accuracy: 0.9140\n",
      "Epoch 98:\n",
      "Training loss: 0.1880, Training accuracy: 0.9577\n",
      "Validation loss: 0.2668, Validation accuracy: 0.9126\n",
      "Epoch 99:\n",
      "Training loss: 0.1869, Training accuracy: 0.9589\n",
      "Validation loss: 0.2773, Validation accuracy: 0.9094\n",
      "Epoch 100:\n",
      "Training loss: 0.1885, Training accuracy: 0.9570\n",
      "Validation loss: 0.2869, Validation accuracy: 0.9045\n",
      "Epoch 101:\n",
      "Training loss: 0.1881, Training accuracy: 0.9574\n",
      "Validation loss: 0.2762, Validation accuracy: 0.9076\n",
      "Epoch 102:\n",
      "Training loss: 0.1867, Training accuracy: 0.9579\n",
      "Validation loss: 0.2825, Validation accuracy: 0.9076\n",
      "Epoch 103:\n",
      "Training loss: 0.1871, Training accuracy: 0.9581\n",
      "Validation loss: 0.2852, Validation accuracy: 0.9042\n",
      "Epoch 104:\n",
      "Training loss: 0.1872, Training accuracy: 0.9579\n",
      "Validation loss: 0.2760, Validation accuracy: 0.9083\n",
      "Epoch 105:\n",
      "Training loss: 0.1894, Training accuracy: 0.9577\n",
      "Validation loss: 0.2811, Validation accuracy: 0.9086\n",
      "Epoch 106:\n",
      "Training loss: 0.1868, Training accuracy: 0.9587\n",
      "Validation loss: 0.2938, Validation accuracy: 0.9010\n",
      "Epoch 107:\n",
      "Training loss: 0.1868, Training accuracy: 0.9589\n",
      "Validation loss: 0.2944, Validation accuracy: 0.9072\n",
      "Epoch 108:\n",
      "Training loss: 0.1872, Training accuracy: 0.9581\n",
      "Validation loss: 0.3104, Validation accuracy: 0.8980\n",
      "Epoch 109:\n",
      "Training loss: 0.1903, Training accuracy: 0.9568\n",
      "Validation loss: 0.3104, Validation accuracy: 0.8981\n",
      "Epoch 110:\n",
      "Training loss: 0.1900, Training accuracy: 0.9574\n",
      "Validation loss: 0.2723, Validation accuracy: 0.9129\n",
      "Epoch 111:\n",
      "Training loss: 0.1893, Training accuracy: 0.9572\n",
      "Validation loss: 0.2852, Validation accuracy: 0.9063\n",
      "Epoch 112:\n",
      "Training loss: 0.1885, Training accuracy: 0.9583\n",
      "Validation loss: 0.3105, Validation accuracy: 0.8995\n",
      "Epoch 113:\n",
      "Training loss: 0.1888, Training accuracy: 0.9577\n",
      "Validation loss: 0.2831, Validation accuracy: 0.9086\n",
      "Epoch 114:\n",
      "Training loss: 0.1907, Training accuracy: 0.9565\n",
      "Validation loss: 0.2836, Validation accuracy: 0.9065\n",
      "Epoch 115:\n",
      "Training loss: 0.1881, Training accuracy: 0.9579\n",
      "Validation loss: 0.2956, Validation accuracy: 0.9020\n",
      "Epoch 116:\n",
      "Training loss: 0.1920, Training accuracy: 0.9565\n",
      "Validation loss: 0.2797, Validation accuracy: 0.9073\n",
      "Epoch 117:\n",
      "Training loss: 0.1881, Training accuracy: 0.9584\n",
      "Validation loss: 0.2829, Validation accuracy: 0.9069\n",
      "Epoch 118:\n",
      "Training loss: 0.1876, Training accuracy: 0.9586\n",
      "Validation loss: 0.2763, Validation accuracy: 0.9104\n",
      "Epoch 119:\n",
      "Training loss: 0.1889, Training accuracy: 0.9575\n",
      "Validation loss: 0.3037, Validation accuracy: 0.9012\n",
      "Epoch 120:\n",
      "Training loss: 0.1914, Training accuracy: 0.9567\n",
      "Validation loss: 0.3053, Validation accuracy: 0.9004\n",
      "Epoch 121:\n",
      "Training loss: 0.1899, Training accuracy: 0.9578\n",
      "Validation loss: 0.2775, Validation accuracy: 0.9088\n",
      "Epoch 122:\n",
      "Training loss: 0.1896, Training accuracy: 0.9574\n",
      "Validation loss: 0.2981, Validation accuracy: 0.9031\n",
      "Epoch 123:\n",
      "Training loss: 0.1917, Training accuracy: 0.9559\n",
      "Validation loss: 0.2883, Validation accuracy: 0.9074\n",
      "Epoch 124:\n",
      "Training loss: 0.1901, Training accuracy: 0.9576\n",
      "Validation loss: 0.2953, Validation accuracy: 0.9048\n",
      "Epoch 125:\n",
      "Training loss: 0.1908, Training accuracy: 0.9573\n",
      "Validation loss: 0.3168, Validation accuracy: 0.8960\n",
      "Epoch 126:\n",
      "Training loss: 0.1923, Training accuracy: 0.9549\n",
      "Validation loss: 0.2931, Validation accuracy: 0.9043\n",
      "Epoch 127:\n",
      "Training loss: 0.1903, Training accuracy: 0.9568\n",
      "Validation loss: 0.3061, Validation accuracy: 0.9017\n",
      "Epoch 128:\n",
      "Training loss: 0.1908, Training accuracy: 0.9573\n",
      "Validation loss: 0.2932, Validation accuracy: 0.9041\n",
      "Epoch 129:\n",
      "Training loss: 0.1924, Training accuracy: 0.9561\n",
      "Validation loss: 0.2855, Validation accuracy: 0.9078\n",
      "Epoch 130:\n",
      "Training loss: 0.1910, Training accuracy: 0.9565\n",
      "Validation loss: 0.2919, Validation accuracy: 0.9017\n",
      "Epoch 131:\n",
      "Training loss: 0.1920, Training accuracy: 0.9568\n",
      "Validation loss: 0.2822, Validation accuracy: 0.9080\n",
      "Epoch 132:\n",
      "Training loss: 0.1896, Training accuracy: 0.9578\n",
      "Validation loss: 0.3027, Validation accuracy: 0.9003\n",
      "Epoch 133:\n",
      "Training loss: 0.1907, Training accuracy: 0.9567\n",
      "Validation loss: 0.3070, Validation accuracy: 0.8996\n",
      "Epoch 134:\n",
      "Training loss: 0.1934, Training accuracy: 0.9550\n",
      "Validation loss: 0.2742, Validation accuracy: 0.9091\n",
      "Epoch 135:\n",
      "Training loss: 0.1880, Training accuracy: 0.9578\n",
      "Validation loss: 0.3057, Validation accuracy: 0.9018\n",
      "Epoch 136:\n",
      "Training loss: 0.1889, Training accuracy: 0.9582\n",
      "Validation loss: 0.2904, Validation accuracy: 0.9049\n",
      "Epoch 137:\n",
      "Training loss: 0.1937, Training accuracy: 0.9554\n",
      "Validation loss: 0.3056, Validation accuracy: 0.9011\n",
      "Epoch 138:\n",
      "Training loss: 0.1902, Training accuracy: 0.9585\n",
      "Validation loss: 0.2760, Validation accuracy: 0.9082\n",
      "Epoch 139:\n",
      "Training loss: 0.1892, Training accuracy: 0.9571\n",
      "Validation loss: 0.3082, Validation accuracy: 0.8959\n",
      "Epoch 140:\n",
      "Training loss: 0.1584, Training accuracy: 0.9718\n",
      "Validation loss: 0.2400, Validation accuracy: 0.9216\n",
      "Saving ...\n",
      "Epoch 141:\n",
      "Training loss: 0.1454, Training accuracy: 0.9779\n",
      "Validation loss: 0.2376, Validation accuracy: 0.9228\n",
      "Saving ...\n",
      "Epoch 142:\n",
      "Training loss: 0.1416, Training accuracy: 0.9797\n",
      "Validation loss: 0.2381, Validation accuracy: 0.9219\n",
      "Epoch 143:\n",
      "Training loss: 0.1402, Training accuracy: 0.9805\n",
      "Validation loss: 0.2363, Validation accuracy: 0.9223\n",
      "Epoch 144:\n",
      "Training loss: 0.1375, Training accuracy: 0.9822\n",
      "Validation loss: 0.2346, Validation accuracy: 0.9232\n",
      "Saving ...\n",
      "Epoch 145:\n",
      "Training loss: 0.1372, Training accuracy: 0.9818\n",
      "Validation loss: 0.2371, Validation accuracy: 0.9238\n",
      "Saving ...\n",
      "Epoch 146:\n",
      "Training loss: 0.1356, Training accuracy: 0.9833\n",
      "Validation loss: 0.2362, Validation accuracy: 0.9213\n",
      "Epoch 147:\n",
      "Training loss: 0.1339, Training accuracy: 0.9837\n",
      "Validation loss: 0.2371, Validation accuracy: 0.9230\n",
      "Epoch 148:\n",
      "Training loss: 0.1334, Training accuracy: 0.9848\n",
      "Validation loss: 0.2349, Validation accuracy: 0.9228\n",
      "Epoch 149:\n",
      "Training loss: 0.1328, Training accuracy: 0.9833\n",
      "Validation loss: 0.2378, Validation accuracy: 0.9218\n",
      "Epoch 150:\n",
      "Training loss: 0.1307, Training accuracy: 0.9849\n",
      "Validation loss: 0.2352, Validation accuracy: 0.9233\n",
      "Epoch 151:\n",
      "Training loss: 0.1311, Training accuracy: 0.9850\n",
      "Validation loss: 0.2373, Validation accuracy: 0.9220\n",
      "Epoch 152:\n",
      "Training loss: 0.1285, Training accuracy: 0.9866\n",
      "Validation loss: 0.2353, Validation accuracy: 0.9232\n",
      "Epoch 153:\n",
      "Training loss: 0.1295, Training accuracy: 0.9856\n",
      "Validation loss: 0.2363, Validation accuracy: 0.9238\n",
      "Epoch 154:\n",
      "Training loss: 0.1287, Training accuracy: 0.9863\n",
      "Validation loss: 0.2347, Validation accuracy: 0.9222\n",
      "Epoch 155:\n",
      "Training loss: 0.1292, Training accuracy: 0.9861\n",
      "Validation loss: 0.2352, Validation accuracy: 0.9238\n",
      "Epoch 156:\n",
      "Training loss: 0.1281, Training accuracy: 0.9860\n",
      "Validation loss: 0.2348, Validation accuracy: 0.9226\n",
      "Epoch 157:\n",
      "Training loss: 0.1281, Training accuracy: 0.9862\n",
      "Validation loss: 0.2361, Validation accuracy: 0.9231\n",
      "Epoch 158:\n",
      "Training loss: 0.1270, Training accuracy: 0.9870\n",
      "Validation loss: 0.2338, Validation accuracy: 0.9247\n",
      "Saving ...\n",
      "Epoch 159:\n",
      "Training loss: 0.1263, Training accuracy: 0.9869\n",
      "Validation loss: 0.2370, Validation accuracy: 0.9233\n",
      "Epoch 160:\n",
      "Training loss: 0.1262, Training accuracy: 0.9877\n",
      "Validation loss: 0.2364, Validation accuracy: 0.9239\n",
      "Epoch 161:\n",
      "Training loss: 0.1266, Training accuracy: 0.9869\n",
      "Validation loss: 0.2365, Validation accuracy: 0.9233\n",
      "Epoch 162:\n",
      "Training loss: 0.1261, Training accuracy: 0.9874\n",
      "Validation loss: 0.2359, Validation accuracy: 0.9213\n",
      "Epoch 163:\n",
      "Training loss: 0.1263, Training accuracy: 0.9875\n",
      "Validation loss: 0.2375, Validation accuracy: 0.9237\n",
      "Epoch 164:\n",
      "Training loss: 0.1254, Training accuracy: 0.9880\n",
      "Validation loss: 0.2364, Validation accuracy: 0.9242\n",
      "Epoch 165:\n",
      "Training loss: 0.1251, Training accuracy: 0.9880\n",
      "Validation loss: 0.2355, Validation accuracy: 0.9233\n",
      "Epoch 166:\n",
      "Training loss: 0.1249, Training accuracy: 0.9881\n",
      "Validation loss: 0.2388, Validation accuracy: 0.9220\n",
      "Epoch 167:\n",
      "Training loss: 0.1247, Training accuracy: 0.9883\n",
      "Validation loss: 0.2356, Validation accuracy: 0.9233\n",
      "Epoch 168:\n",
      "Training loss: 0.1246, Training accuracy: 0.9889\n",
      "Validation loss: 0.2354, Validation accuracy: 0.9235\n",
      "Epoch 169:\n",
      "Training loss: 0.1241, Training accuracy: 0.9888\n",
      "Validation loss: 0.2379, Validation accuracy: 0.9230\n",
      "Epoch 170:\n",
      "Training loss: 0.1228, Training accuracy: 0.9891\n",
      "Validation loss: 0.2356, Validation accuracy: 0.9240\n",
      "Epoch 171:\n",
      "Training loss: 0.1249, Training accuracy: 0.9880\n",
      "Validation loss: 0.2363, Validation accuracy: 0.9250\n",
      "Saving ...\n",
      "Epoch 172:\n",
      "Training loss: 0.1240, Training accuracy: 0.9885\n",
      "Validation loss: 0.2368, Validation accuracy: 0.9223\n",
      "Epoch 173:\n",
      "Training loss: 0.1227, Training accuracy: 0.9897\n",
      "Validation loss: 0.2374, Validation accuracy: 0.9221\n",
      "Epoch 174:\n",
      "Training loss: 0.1229, Training accuracy: 0.9894\n",
      "Validation loss: 0.2373, Validation accuracy: 0.9220\n",
      "Epoch 175:\n",
      "Training loss: 0.1225, Training accuracy: 0.9899\n",
      "Validation loss: 0.2361, Validation accuracy: 0.9239\n",
      "Epoch 176:\n",
      "Training loss: 0.1223, Training accuracy: 0.9892\n",
      "Validation loss: 0.2382, Validation accuracy: 0.9231\n",
      "Epoch 177:\n",
      "Training loss: 0.1220, Training accuracy: 0.9898\n",
      "Validation loss: 0.2387, Validation accuracy: 0.9238\n",
      "Epoch 178:\n",
      "Training loss: 0.1221, Training accuracy: 0.9901\n",
      "Validation loss: 0.2375, Validation accuracy: 0.9235\n",
      "Epoch 179:\n",
      "Training loss: 0.1216, Training accuracy: 0.9899\n",
      "Validation loss: 0.2387, Validation accuracy: 0.9229\n",
      "Epoch 180:\n",
      "Training loss: 0.1216, Training accuracy: 0.9898\n",
      "Validation loss: 0.2401, Validation accuracy: 0.9209\n",
      "Epoch 181:\n",
      "Training loss: 0.1212, Training accuracy: 0.9899\n",
      "Validation loss: 0.2410, Validation accuracy: 0.9219\n",
      "Epoch 182:\n",
      "Training loss: 0.1220, Training accuracy: 0.9890\n",
      "Validation loss: 0.2395, Validation accuracy: 0.9231\n",
      "Epoch 183:\n",
      "Training loss: 0.1209, Training accuracy: 0.9897\n",
      "Validation loss: 0.2394, Validation accuracy: 0.9221\n",
      "Epoch 184:\n",
      "Training loss: 0.1211, Training accuracy: 0.9902\n",
      "Validation loss: 0.2410, Validation accuracy: 0.9233\n",
      "Epoch 185:\n",
      "Training loss: 0.1207, Training accuracy: 0.9906\n",
      "Validation loss: 0.2398, Validation accuracy: 0.9223\n",
      "Epoch 186:\n",
      "Training loss: 0.1199, Training accuracy: 0.9907\n",
      "Validation loss: 0.2407, Validation accuracy: 0.9210\n",
      "Epoch 187:\n",
      "Training loss: 0.1206, Training accuracy: 0.9904\n",
      "Validation loss: 0.2400, Validation accuracy: 0.9224\n",
      "Epoch 188:\n",
      "Training loss: 0.1196, Training accuracy: 0.9903\n",
      "Validation loss: 0.2401, Validation accuracy: 0.9230\n",
      "Epoch 189:\n",
      "Training loss: 0.1200, Training accuracy: 0.9900\n",
      "Validation loss: 0.2388, Validation accuracy: 0.9225\n",
      "Epoch 190:\n",
      "Training loss: 0.1198, Training accuracy: 0.9906\n",
      "Validation loss: 0.2412, Validation accuracy: 0.9217\n",
      "Epoch 191:\n",
      "Training loss: 0.1195, Training accuracy: 0.9907\n",
      "Validation loss: 0.2414, Validation accuracy: 0.9225\n",
      "Epoch 192:\n",
      "Training loss: 0.1198, Training accuracy: 0.9911\n",
      "Validation loss: 0.2398, Validation accuracy: 0.9237\n",
      "Epoch 193:\n",
      "Training loss: 0.1188, Training accuracy: 0.9910\n",
      "Validation loss: 0.2399, Validation accuracy: 0.9226\n",
      "Epoch 194:\n",
      "Training loss: 0.1193, Training accuracy: 0.9908\n",
      "Validation loss: 0.2371, Validation accuracy: 0.9240\n",
      "Epoch 195:\n",
      "Training loss: 0.1193, Training accuracy: 0.9908\n",
      "Validation loss: 0.2374, Validation accuracy: 0.9236\n",
      "Epoch 196:\n",
      "Training loss: 0.1176, Training accuracy: 0.9914\n",
      "Validation loss: 0.2380, Validation accuracy: 0.9223\n",
      "Epoch 197:\n",
      "Training loss: 0.1188, Training accuracy: 0.9905\n",
      "Validation loss: 0.2390, Validation accuracy: 0.9234\n",
      "Epoch 198:\n",
      "Training loss: 0.1188, Training accuracy: 0.9911\n",
      "Validation loss: 0.2389, Validation accuracy: 0.9234\n",
      "Epoch 199:\n",
      "Training loss: 0.1183, Training accuracy: 0.9911\n",
      "Validation loss: 0.2401, Validation accuracy: 0.9232\n",
      "==================================================\n",
      "==> Optimization finished! Best validation accuracy: 0.9250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.925"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(4,[1,0,0],0.2,train_loader,val_loader,EPOCHS=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "55f20d8859a1a0149f7d957af872e078c8283691172f5ca78f1d0b16a2e38dc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
