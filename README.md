# ECE661_knowledge_distilation_project_minke-ziling
#### MNIST_teacher_train.ipynb : The process of teacher model trainning on MNIST.
#### MNIST_student_train.ipynb : The process of student model training on MNIST. (baseline,ablation)
#### MNIST_distil.ipynb : The experiment of knowledge distillation on MNIST.
#### MNIST_distil_reverse.ipynb : The reverse distillation experiment.
#### MNIST_distil_remove1class.ipynb : The experiment of knowledge distillation with one digit class removed.
#### resnet-cifar10.ipynb : The process of teacher model trainning on CIFAR10.
#### resnet9-cifar10.ipynb : The process of student model training on CIFAR10. (baseline, ablation)
#### resnet_cifar10_distil.ipynb : The experiment of knowledge distillation on CIFAR10.
#### resnet50_cifar10.ipynb : The process of resnet50 trainning on CIFAR10. (baseline, ablation)
#### resnet_cifar10_self_distil.ipynb : Resnet50 self distilation experiment.
#### resnet_cifar10_distil_reverse.ipynb : Experiment for reverse distillation on cifar10.

## References
### [1] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. “Distilling the knowledge in a neural network”. In: arXiv preprint arXiv:1503.02531 2.7 (2015).
### [2] Nicolas Papernot et al. “Distillation as a defense to adversarial perturbations against deep neural networks”. In: 2016 IEEE symposium on security and privacy (SP). IEEE. 2016, pp. 582–597.
### [3] Linfeng Zhang et al. “Be your own teacher: Improve the performance of convolutional neuralnetworks via self distillation”. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019, pp. 3713–3722.